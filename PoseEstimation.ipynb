{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install and import mediapipe and opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install mediapipe opencv-python \n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils #gives us drawing utilities\n",
    "mp_pose = mp.solutions.pose #the pose estimation model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're gonna be using the below block of code again, it's a basic framework to take webcam input and show it in a window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting a real time video feed from our webcam\n",
    "cap = cv2.VideoCapture(0)   #capturing video from our webcam: the parameter is usually 0, otherwise chnge it around and see\n",
    "while cap.isOpened():\n",
    "    frame = cap.read()[1]   #reading what you've captured from webcam, it returns return value and the frame content\n",
    "    cv2.imshow('Mediapipe Feed',frame)  #showing it in a window\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):  #taking the 0xFF bits of keyboard input(ord gives unicode of character): if q close after 10 millisecs\n",
    "        break\n",
    "\n",
    "cap.release()   #release the video feed capturing\n",
    "cv2.destroyAllWindows() #close windows opened\n",
    "\n",
    "#it didn't close even when i pressed x button, only when i entered q(not Q)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)  \n",
    "\n",
    "# setting up a mediapipe instance mp_pose was defined earlier remember\n",
    "with mp_pose.Pose(min_detection_confidence=0.5,min_tracking_confidence=0.5) as pose:    #this confidence levels can be increased as per convenvience\n",
    "    while cap.isOpened():\n",
    "        frame = cap.read()[1]   #datatype is numpy array\n",
    "\n",
    "        image = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)   #mediapipe requires RBG, but when captured its BGR\n",
    "        image.flags.writeable = False   #to make the RGB colour arrays editable i think\n",
    "\n",
    "        results = pose.process(image)   #access our pose model to process our image and store the detections in results\n",
    "\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  #bring it back to BGR so we can rerender it using cv2\n",
    "\n",
    "        #now we draw on our image with the landmarks: all the body tracking points, and then add the connections between thme as well\n",
    "        # and change the colours(BGR) for landmark first and then connection as well\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "            mp_drawing.DrawingSpec(color=(255,0,0),thickness=2,circle_radius=2),\n",
    "            mp_drawing.DrawingSpec(color=(0,255,0),thickness=2,circle_radius=2)\n",
    "            )\n",
    "\n",
    "        cv2.imshow('Mediapipe Feed',image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mp_drawing.draw_landmarks?? \n",
    "\n",
    "#gives deets regarding this object"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determining Joints"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build upon the same code and just get the landmarks and their coordinates in a try except statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)  \n",
    "\n",
    "# i increased the confidence\n",
    "with mp_pose.Pose(min_detection_confidence=0.5,min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        frame = cap.read()[1]\n",
    "\n",
    "        image = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        results = pose.process(image)\n",
    "\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # get the landmarks(this is what we passed to draw below), \n",
    "        # some joints might be out of camera etc, so otherwie pass\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "            mp_drawing.DrawingSpec(color=(255,0,0),thickness=2,circle_radius=2),\n",
    "            mp_drawing.DrawingSpec(color=(0,255,0),thickness=2,circle_radius=2)\n",
    "            )\n",
    "\n",
    "        cv2.imshow('Mediapipe Feed',image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see that there are 33 landmarks and they are as follows. This allows us to choose the ones we require."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PoseLandmark.NOSE 0\n",
      "PoseLandmark.LEFT_EYE_INNER 1\n",
      "PoseLandmark.LEFT_EYE 2\n",
      "PoseLandmark.LEFT_EYE_OUTER 3\n",
      "PoseLandmark.RIGHT_EYE_INNER 4\n",
      "PoseLandmark.RIGHT_EYE 5\n",
      "PoseLandmark.RIGHT_EYE_OUTER 6\n",
      "PoseLandmark.LEFT_EAR 7\n",
      "PoseLandmark.RIGHT_EAR 8\n",
      "PoseLandmark.MOUTH_LEFT 9\n",
      "PoseLandmark.MOUTH_RIGHT 10\n",
      "PoseLandmark.LEFT_SHOULDER 11\n",
      "PoseLandmark.RIGHT_SHOULDER 12\n",
      "PoseLandmark.LEFT_ELBOW 13\n",
      "PoseLandmark.RIGHT_ELBOW 14\n",
      "PoseLandmark.LEFT_WRIST 15\n",
      "PoseLandmark.RIGHT_WRIST 16\n",
      "PoseLandmark.LEFT_PINKY 17\n",
      "PoseLandmark.RIGHT_PINKY 18\n",
      "PoseLandmark.LEFT_INDEX 19\n",
      "PoseLandmark.RIGHT_INDEX 20\n",
      "PoseLandmark.LEFT_THUMB 21\n",
      "PoseLandmark.RIGHT_THUMB 22\n",
      "PoseLandmark.LEFT_HIP 23\n",
      "PoseLandmark.RIGHT_HIP 24\n",
      "PoseLandmark.LEFT_KNEE 25\n",
      "PoseLandmark.RIGHT_KNEE 26\n",
      "PoseLandmark.LEFT_ANKLE 27\n",
      "PoseLandmark.RIGHT_ANKLE 28\n",
      "PoseLandmark.LEFT_HEEL 29\n",
      "PoseLandmark.RIGHT_HEEL 30\n",
      "PoseLandmark.LEFT_FOOT_INDEX 31\n",
      "PoseLandmark.RIGHT_FOOT_INDEX 32\n"
     ]
    }
   ],
   "source": [
    "for lndmrk in mp_pose.PoseLandmark:\n",
    "    print(lndmrk, lndmrk.value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to get the coordinates of a particular landmark we do the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 0.7357970476150513\n",
      "y: 0.8371623158454895\n",
      "z: -0.07982823997735977\n",
      "visibility: 0.9987908005714417\n",
      "\n",
      "x: 0.7357970476150513\n",
      "y: 0.8371623158454895\n",
      "z: -0.07982823997735977\n",
      "visibility: 0.9987908005714417\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value])\n",
    "#or\n",
    "print(landmarks[11])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle(a,b,c):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    c = np.array(c)\n",
    "\n",
    "    radians = np.arctan2(c[1]-b[1],c[0]-b[0]) - np.arctan2(a[1]-b[1],a[0]-b[0])\n",
    "    angle = np.abs(radians*180.0/np.pi)\n",
    "\n",
    "    if angle > 180.0:\n",
    "        angle = 360 - angle\n",
    "\n",
    "    return angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "shoulder_left = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "elbow_left = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x,landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "wrist_left = [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x,landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.454702929966423"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_angle(shoulder_left,elbow_left,wrist_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)  \n",
    "\n",
    "# i decreased the confidence because landmarks weren't generating\n",
    "with mp_pose.Pose(min_detection_confidence=0.7,min_tracking_confidence=0.7) as pose:\n",
    "    while cap.isOpened():\n",
    "        frame = cap.read()[1]\n",
    "\n",
    "        image = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        results = pose.process(image)\n",
    "\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "            # get coordinates for angle calculation\n",
    "            shoulder_right = [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x,landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y]\n",
    "            elbow_right = [landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].x,landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].y]\n",
    "            wrist_right = [landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].x,landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].y]\n",
    "\n",
    "            thumb_right = [landmarks[mp_pose.PoseLandmark.RIGHT_THUMB.value].x,landmarks[mp_pose.PoseLandmark.RIGHT_THUMB.value].y]\n",
    "            # wrist_right = [landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].x,landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].y]\n",
    "            # elbow_right = [landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].x,landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].y]\n",
    "\n",
    "            angle1 = calculate_angle(shoulder_right, elbow_right, wrist_right)\n",
    "            angle2 = calculate_angle(thumb_right, wrist_right, elbow_right)\n",
    "\n",
    "            # visualize this angle by showing angle value on screen as a string\n",
    "            # the coordinates we have are relative coordinates so we multiply them with\n",
    "            # the size of our window            \n",
    "            cv2.putText(image, str(angle1),\n",
    "                    tuple(np.multiply(elbow_right, [640,480]).astype(int)),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 2, cv2.LINE_AA )\n",
    "            cv2.putText(image, str(angle2),\n",
    "                    tuple(np.multiply(wrist_right, [640,480]).astype(int)),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 2, cv2.LINE_AA )\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "            mp_drawing.DrawingSpec(color=(255,0,0),thickness=2,circle_radius=2),\n",
    "            mp_drawing.DrawingSpec(color=(0,255,0),thickness=2,circle_radius=2)\n",
    "            )\n",
    "\n",
    "        cv2.imshow('Mediapipe Feed',image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be mindful of the midpoint and text on the midpoint when you change the landmark to find angles for different joints"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Curl counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)  \n",
    "\n",
    "#add a count variable and a stage variable\n",
    "counter = 0\n",
    "stage = None\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.7,min_tracking_confidence=0.7) as pose:\n",
    "    while cap.isOpened():\n",
    "        frame = cap.read()[1]\n",
    "\n",
    "        image = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        results = pose.process(image)\n",
    "\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "            shoulder_right = [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x,landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y]\n",
    "            elbow_right = [landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].x,landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].y]\n",
    "            wrist_right = [landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].x,landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].y]\n",
    "\n",
    "            thumb_right = [landmarks[mp_pose.PoseLandmark.RIGHT_THUMB.value].x,landmarks[mp_pose.PoseLandmark.RIGHT_THUMB.value].y]\n",
    "\n",
    "            angle1 = calculate_angle(shoulder_right, elbow_right, wrist_right)\n",
    "            # angle2 = calculate_angle(thumb_right, wrist_right, elbow_right)\n",
    "\n",
    "                       \n",
    "            cv2.putText(image, str(angle1),\n",
    "                    tuple(np.multiply(elbow_right, [640,480]).astype(int)),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 2, cv2.LINE_AA )\n",
    "            # cv2.putText(image, str(angle2),\n",
    "            #         tuple(np.multiply(wrist_right, [640,480]).astype(int)),\n",
    "            #         cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 2, cv2.LINE_AA )\n",
    "\n",
    "            # curl counter based on angles\n",
    "            if angle1 > 160:\n",
    "                stage = \"down\"\n",
    "            elif angle1 < 40 and stage == 'down':\n",
    "                stage = \"up\"\n",
    "                counter += 1\n",
    "                print(counter)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Render curl counter: status box is placed in top left corner\n",
    "        # parameters are image, start and end point of rectangle, color, line width/thickness (-1 fills the box)\n",
    "        cv2.rectangle(image, (0,0), (225, 73), (245,117,16), -1)\n",
    "\n",
    "        # Data about the curls\n",
    "        # image, text we want to display, start coordinate of text, font, size etc. and same for counter\n",
    "        cv2.putText(image, 'REPS', (15,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1, cv2.LINE_AA)\n",
    "        cv2.putText(image, str(counter), (10,60), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255,255,255), 2, cv2.LINE_AA)\n",
    "\n",
    "        #stage of rep also is displayed as below\n",
    "        cv2.putText(image, 'STAGE', (65,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1, cv2.LINE_AA)\n",
    "        cv2.putText(image, stage, (70,60), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255,255,255), 2, cv2.LINE_AA)\n",
    "\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "            mp_drawing.DrawingSpec(color=(255,0,0),thickness=2,circle_radius=2),\n",
    "            mp_drawing.DrawingSpec(color=(0,255,0),thickness=2,circle_radius=2)\n",
    "            )\n",
    "\n",
    "        cv2.imshow('Mediapipe Feed',image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "rectangle(img, pt1, pt2, color[, thickness[, lineType[, shift]]]) -> img\n",
      ".   @brief Draws a simple, thick, or filled up-right rectangle.\n",
      ".   \n",
      ".   The function cv::rectangle draws a rectangle outline or a filled rectangle whose two opposite corners\n",
      ".   are pt1 and pt2.\n",
      ".   \n",
      ".   @param img Image.\n",
      ".   @param pt1 Vertex of the rectangle.\n",
      ".   @param pt2 Vertex of the rectangle opposite to pt1 .\n",
      ".   @param color Rectangle color or brightness (grayscale image).\n",
      ".   @param thickness Thickness of lines that make up the rectangle. Negative values, like #FILLED,\n",
      ".   mean that the function has to draw a filled rectangle.\n",
      ".   @param lineType Type of the line. See #LineTypes\n",
      ".   @param shift Number of fractional bits in the point coordinates.\n",
      "\n",
      "\n",
      "\n",
      "rectangle(img, rec, color[, thickness[, lineType[, shift]]]) -> img\n",
      ".   @overload\n",
      ".   \n",
      ".   use `rec` parameter as alternative specification of the drawn rectangle: `r.tl() and\n",
      ".   r.br()-Point(1,1)` are opposite corners\n",
      "\u001b[1;31mType:\u001b[0m      builtin_function_or_method"
     ]
    }
   ],
   "source": [
    "#cv2.rectangle??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupy_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae697f20fda1e6d7520a8ac0e590ee0536f901bb12110b3b5576d5bd7fe4f6fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
